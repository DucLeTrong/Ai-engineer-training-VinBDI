{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JaVi_Translation_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYBTAx79pPAX"
      },
      "source": [
        "from torch.utils.data import IterableDataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from collections import defaultdict\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "\r\n",
        "import gensim\r\n",
        "\r\n",
        "import math\r\n",
        "import time\r\n",
        "import tqdm\r\n",
        "import pickle\r\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJySVEQWpWQe"
      },
      "source": [
        "def vi_preprocessor(text):\r\n",
        "    return [t.lower() for t in text.replace('.','').replace(',','').split()]\r\n",
        "\r\n",
        "def ja_preprocessor(text):\r\n",
        "    return [t.lower() for t in text.replace('.','').replace(',','').split()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1VAa6yZsJNH"
      },
      "source": [
        "##collator function\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlnCR93cpbkJ"
      },
      "source": [
        "def collator(batch, PAD_IDX, max_src_len, max_trg_len):\r\n",
        "    dyn_max_src_len = dyn_max_trg_len = 0\r\n",
        "    for x, y in batch:\r\n",
        "        dyn_max_src_len = max(dyn_max_src_len, len(x))\r\n",
        "        dyn_max_trg_len = max(dyn_max_trg_len, len(y))\r\n",
        "    \r\n",
        "    dyn_max_src_len = min(dyn_max_src_len, max_src_len)\r\n",
        "    dyn_max_trg_len = min(dyn_max_trg_len, max_trg_len)\r\n",
        "\r\n",
        "    X = []\r\n",
        "    X_len = []\r\n",
        "    Y = []\r\n",
        "    for x, y in batch:\r\n",
        "        X.append(x[:dyn_max_src_len] + [PAD_IDX for i in range(max(dyn_max_src_len-len(x), 0))])\r\n",
        "        X_len.append(min(len(x), dyn_max_src_len))\r\n",
        "        Y.append(y[:dyn_max_trg_len] + [PAD_IDX for i in range(max(dyn_max_trg_len-len(y), 0))])\r\n",
        "\r\n",
        "    Y = torch.tensor(Y).contiguous()\r\n",
        "    X = torch.tensor(X).contiguous()\r\n",
        "\r\n",
        "    X_len = torch.tensor(X_len)\r\n",
        "    return (X, X_len), Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Iz4IMVsObC"
      },
      "source": [
        "#Vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxek_5_ksHdm"
      },
      "source": [
        "class Vocab:\r\n",
        "    def __init__(self, src_dic=None, trg_dic=None):\r\n",
        "        self.src_stoi = src_dic\r\n",
        "        self.src_itos = {}\r\n",
        "        self.word_vec = {}\r\n",
        "        if self.src_stoi is not None:\r\n",
        "            for k, v in self.src_stoi.items():\r\n",
        "                self.src_itos[v] = k\r\n",
        "        self.trg_stoi = trg_dic\r\n",
        "        self.trg_itos = {}\r\n",
        "\r\n",
        "        if self.trg_stoi is not None:\r\n",
        "            for k, v in self.trg_stoi.items():\r\n",
        "                self.trg_itos[v] = k\r\n",
        "\r\n",
        "    def ret_z(self):\r\n",
        "        return 0\r\n",
        "\r\n",
        "    def build_dic(self, path, preprocessor, vocab_size, lang='ja'):\r\n",
        "        dic = {}\r\n",
        "        freq_dic = defaultdict(self.ret_z)\r\n",
        "\r\n",
        "        dic['<UNK>'] = 0\r\n",
        "        dic['<sos>'] = 1\r\n",
        "        dic['<eos>'] = 2\r\n",
        "        dic['<pad>'] = 3\r\n",
        "\r\n",
        "        ctr = 4\r\n",
        "        with open(path, 'r', encoding='utf-8') as F:\r\n",
        "            for line in F:\r\n",
        "                tokens = preprocessor(line)\r\n",
        "                for token in tokens:\r\n",
        "                    freq_dic[token] += 1\r\n",
        "        for k, v in sorted(freq_dic.items(), key = lambda kv:(kv[1],kv[0]), reverse=True):\r\n",
        "            if k not in dic:\r\n",
        "                dic[k] = ctr\r\n",
        "                ctr += 1\r\n",
        "                if ctr == vocab_size:\r\n",
        "                    break\r\n",
        "        vec_dic = self.build_vector(dic, lang)\r\n",
        "        return dic, vec_dic\r\n",
        "\r\n",
        "    def build_vector(self, dic, lang='ja'):\r\n",
        "        full_dic = {}\r\n",
        "        vec_dic = {}\r\n",
        "        if lang == 'ja':\r\n",
        "            f = open('/content/drive/MyDrive/JaVi_Translation/w2v_pretrained/ja_w2v.txt', encoding='utf-8')\r\n",
        "            for line in f.readlines():\r\n",
        "                try:\r\n",
        "                    full_dic[line.split('\\t')[0]] = torch.Tensor([float(x) for x in line.split('\\t')[1].split()])\r\n",
        "                except Exception:\r\n",
        "                    print(line)\r\n",
        "            for w in dic:\r\n",
        "                if w in full_dic:\r\n",
        "                    vec_dic[dic[w]] = full_dic[w]\r\n",
        "                else:\r\n",
        "                    vec_dic[dic[w]] = torch.rand(300)\r\n",
        "            f.close()\r\n",
        "        elif lang == 'vi':\r\n",
        "            wv_model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/JaVi_Translation/w2v_pretrained/baomoi.window2.vn.model.bin', binary=True) \r\n",
        "            for w in dic:\r\n",
        "                if w in wv_model.wv.vocab:\r\n",
        "                    vec_dic[dic[w]] = torch.Tensor(wv_model.wv[w].tolist())    \r\n",
        "                else:\r\n",
        "                    vec_dic[dic[w]] = torch.rand(300)\r\n",
        "        return vec_dic\r\n",
        "\r\n",
        "    def add_src_dic(self, dic):\r\n",
        "        self.src_stoi = dic\r\n",
        "        for k, v in self.src_stoi.items():\r\n",
        "            self.src_itos[v] = k\r\n",
        "\r\n",
        "    def add_trg_dic(self, dic):\r\n",
        "        self.trg_stoi = dic\r\n",
        "        for k, v in self.trg_stoi.items():\r\n",
        "            self.trg_itos[v]=k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldG0uU1U5K1B"
      },
      "source": [
        "#Data Reader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTZCT0rJvujV"
      },
      "source": [
        "class DataReader(IterableDataset):\r\n",
        "    def __init__(self, args, paths, preprocessors, vocab_sizes=(100,100), DIC=None):\r\n",
        "        self.src_path = paths[0]\r\n",
        "        self.trg_path = paths[1]\r\n",
        "\r\n",
        "        self.src_preprocessor = preprocessors[0]\r\n",
        "        self.trg_preprocessor = preprocessors[1]\r\n",
        "\r\n",
        "        self.src_vocab_size = vocab_sizes[0]\r\n",
        "        self.trg_vocab_size = vocab_sizes[1]\r\n",
        "\r\n",
        "        self.vocab = Vocab()\r\n",
        "        if DIC is None:\r\n",
        "            src_dic, self.src_vec_dic = self.vocab.build_dic(self.src_path, self.src_preprocessor, self.src_vocab_size, lang='ja')\r\n",
        "            trg_dic, self.trg_vec_dic = self.vocab.build_dic(self.trg_path, self.trg_preprocessor, self.trg_vocab_size, lang='vi')\r\n",
        "\r\n",
        "            self.vocab.add_src_dic(src_dic)\r\n",
        "            self.vocab.add_trg_dic(trg_dic)\r\n",
        "            self.src_vocab_size = len(self.vocab.src_stoi)\r\n",
        "            self.trg_vocab_size = len(self.vocab.trg_stoi)\r\n",
        "        else:\r\n",
        "            self.vocab = DIC\r\n",
        "        \r\n",
        "    def line_mapper(self, line, is_src):\r\n",
        "        text = line\r\n",
        "        tokens = []\r\n",
        "        if is_src:\r\n",
        "            tokens.append(self.vocab.src_stoi['<sos>'])\r\n",
        "            tokens = tokens + [self.vocab.src_stoi.get(token, 0) for token in self.src_preprocessor(text)]\r\n",
        "            tokens.append(self.vocab.src_stoi['<eos>'])\r\n",
        "        else:\r\n",
        "            tokens.append(self.vocab.trg_stoi['<sos>'])\r\n",
        "            tokens = tokens + [self.vocab.trg_stoi.get(token, 0) for token in self.trg_preprocessor(text)]\r\n",
        "            tokens.append(self.vocab.trg_stoi['<eos>'])\r\n",
        "        return tokens\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        src_itr = open(self.src_path, encoding='utf-8')\r\n",
        "        trg_itr = open(self.trg_path, encoding='utf-8')\r\n",
        "\r\n",
        "        mapped_src_itr = map(lambda text : self.line_mapper(text, True), src_itr)\r\n",
        "        mapped_trg_itr = map(lambda text : self.line_mapper(text, False), trg_itr)\r\n",
        "\r\n",
        "        zipped_itr = zip(mapped_src_itr, mapped_trg_itr)\r\n",
        "\r\n",
        "        return zipped_itr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbqz3ziu_CoP"
      },
      "source": [
        "#MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ3MbVCV_LFC"
      },
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\r\n",
        "        super(MultiHeadAttentionLayer, self).__init__()\r\n",
        "\r\n",
        "        self.hid_dim = hid_dim\r\n",
        "        self.n_heads = n_heads\r\n",
        "        self.head_dim = hid_dim // n_heads\r\n",
        "\r\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\r\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\r\n",
        "\r\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\r\n",
        "\r\n",
        "    def forward(self, query, key, value, mask=None):\r\n",
        "        batch_size = query.shape[0]\r\n",
        "        \r\n",
        "        Q = self.fc_q(query)\r\n",
        "        K = self.fc_k(key)\r\n",
        "        V = self.fc_v(value)\r\n",
        "\r\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\r\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\r\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0,2,1,3)\r\n",
        "        \r\n",
        "        energy = torch.matmul(Q, K.permute(0,1,3,2)) / self.scale\r\n",
        "\r\n",
        "        if mask is not None:\r\n",
        "            energy = energy.masked_fill(mask==0, -1e10)\r\n",
        "        \r\n",
        "        attention = torch.softmax(energy, dim=-1)\r\n",
        "\r\n",
        "        x = torch.matmul(self.dropout(attention), V)\r\n",
        "        x = x.permute(0,2,1,3).contiguous()\r\n",
        "\r\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\r\n",
        "        x = self.fc_o(x)\r\n",
        "        return x, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgChoS-lFxDJ"
      },
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\r\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\r\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\r\n",
        "        x = self.fc_2(x)\r\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UrrZ5mx5n85"
      },
      "source": [
        "#Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyekDQMfE_xQ"
      },
      "source": [
        "class EncoderLayer(nn.Module):\r\n",
        "    def __init__(self,\r\n",
        "                 hid_dim,\r\n",
        "                 n_heads,\r\n",
        "                 pf_dim,\r\n",
        "                 dropout,\r\n",
        "                 device):\r\n",
        "        super(EncoderLayer, self).__init__()\r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "    def forward(self, src, src_mask):\r\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\r\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\r\n",
        "        _src = self.positionwise_feedforward(src)\r\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\r\n",
        "\r\n",
        "        return src\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0IKpyQVzRfy"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 input_vocab_size, \r\n",
        "                 input_emb_dim,\r\n",
        "                 n_layers,\r\n",
        "                 n_heads,\r\n",
        "                 pf_dim,\r\n",
        "                 max_sent_len,\r\n",
        "                 dropout,\r\n",
        "                 device,\r\n",
        "                 emb_weight=None):\r\n",
        "        super(Encoder, self).__init__()\r\n",
        "        self.device = device\r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([input_emb_dim])).to(device)\r\n",
        "        # self.embedding = nn.Embedding.from_pretrained(\r\n",
        "        #         embeddings=torch.as_tensor(emb_weight),\r\n",
        "        #         padding_idx = 3,\r\n",
        "        #         freeze=False\r\n",
        "        #     )\r\n",
        "        self.embedding = nn.Embedding(input_vocab_size, input_emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_sent_len, input_emb_dim)\r\n",
        "        self.layers = nn.ModuleList([EncoderLayer(input_emb_dim,\r\n",
        "                                                  n_heads,\r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout,\r\n",
        "                                                  device) for _ in range(n_layers)])\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, input, mask):\r\n",
        "\r\n",
        "        batch_size = input.shape[0]\r\n",
        "        input_len = input.shape[1]\r\n",
        "\r\n",
        "        pos = torch.arange(0, input_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "\r\n",
        "        word_emb = self.embedding(input)\r\n",
        "        pos_emb = self.pos_embedding(pos)\r\n",
        "\r\n",
        "        src = self.dropout((word_emb*self.scale) + pos_emb)\r\n",
        "        \r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src, mask)\r\n",
        "\r\n",
        "        return src"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EisRljWB8STw"
      },
      "source": [
        "#Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ru61R-7sN3zQ"
      },
      "source": [
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self,\r\n",
        "                 hid_dim,\r\n",
        "                 n_heads,\r\n",
        "                 pf_dim,\r\n",
        "                 dropout,\r\n",
        "                 device):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.enc_attn_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\r\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.encoder_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\r\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout) \r\n",
        "\r\n",
        "    def forward(self, trg, enc_src, trg_mask, src_mask):\r\n",
        "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\r\n",
        "        trg = self.self_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "\r\n",
        "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\r\n",
        "        trg = self.enc_attn_layer_norm(trg + self.dropout(_trg))\r\n",
        "\r\n",
        "        trg = self.enc_attn_layer_norm(trg +self.dropout(_trg))\r\n",
        "        _trg = self.positionwise_feedforward(trg)\r\n",
        "        trg = self.ff_layer_norm(trg + self.dropout(_trg))\r\n",
        "\r\n",
        "        return trg, attention                                                                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvb1Y2cj66aU"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 output_vocab_size, \r\n",
        "                 output_emb_dim, \r\n",
        "                 n_layers,\r\n",
        "                 n_heads,\r\n",
        "                 pf_dim,\r\n",
        "                 max_sent_len,\r\n",
        "                 dropout,\r\n",
        "                 device,\r\n",
        "                 emb_weight=None):\r\n",
        "        super(Decoder, self).__init__()\r\n",
        "        self.device = device \r\n",
        "       \r\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([output_emb_dim])).to(device)\r\n",
        "        # self.embedding = nn.Embedding.from_pretrained(\r\n",
        "        #         embeddings=torch.as_tensor(emb_weight),\r\n",
        "        #         padding_idx = 3,\r\n",
        "        #         freeze=False\r\n",
        "        #     )\r\n",
        "        self.embedding= nn.Embedding(output_vocab_size, output_emb_dim)\r\n",
        "        self.pos_embedding = nn.Embedding(max_sent_len, output_emb_dim)\r\n",
        "\r\n",
        "        self.layers = nn.ModuleList([DecoderLayer(output_emb_dim,\r\n",
        "                                                  n_heads,\r\n",
        "                                                  pf_dim,\r\n",
        "                                                  dropout,\r\n",
        "                                                  device) for _ in range(n_layers)])\r\n",
        "\r\n",
        "        self.fc_out = nn.Linear(output_emb_dim, output_vocab_size)\r\n",
        "\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "        \r\n",
        "    def forward(self, input, enc_src, trg_mask, src_mask):\r\n",
        "        \r\n",
        "        batch_size = input.shape[0]\r\n",
        "        input_len = input.shape[1]\r\n",
        "\r\n",
        "        pos = torch.arange(0, input_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\r\n",
        "\r\n",
        "        word_emb = self.embedding(input)\r\n",
        "        pos_emb = self.pos_embedding(pos)\r\n",
        "\r\n",
        "        trg = self.dropout(word_emb*self.scale + pos_emb)\r\n",
        "        # print(trg.shape)\r\n",
        "        for layer in self.layers:\r\n",
        "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\r\n",
        "        output = self.fc_out(trg)\r\n",
        "\r\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvndcJ-0QL3d"
      },
      "source": [
        "#Seq2Seq\r\n",
        "\r\n",
        "src = [src sent len, batch size] \r\n",
        "\r\n",
        "src_len = [batch size]\r\n",
        " \r\n",
        "trg = [trg sent len, batch size] "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bmDi1woQD1n"
      },
      "source": [
        "class Seq2Seq(nn.Module):\r\n",
        "    def __init__(self, \r\n",
        "                 ags, \r\n",
        "                 input_vocab_size, \r\n",
        "                 output_vocab_size, \r\n",
        "                 pad_idx, \r\n",
        "                 sos_idx, \r\n",
        "                 eos_idx, \r\n",
        "                 src_vec_emb=None, \r\n",
        "                 trg_vec_emb=None):\r\n",
        "        super(Seq2Seq, self).__init__()\r\n",
        "\r\n",
        "        self.input_vocab_size = input_vocab_size\r\n",
        "        self.output_vocab_size = output_vocab_size\r\n",
        "\r\n",
        "        self.pad_idx = pad_idx\r\n",
        "        self.sos_idx = sos_idx\r\n",
        "        self.eos_idx = eos_idx\r\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "        self.encoder = Encoder(input_vocab_size, \r\n",
        "                               args.input_embedding_dim, \r\n",
        "                               args.n_layers, \r\n",
        "                               args.n_heads,\r\n",
        "                               args.pf_dim, \r\n",
        "                               args.max_sent_len, \r\n",
        "                               args.dropout, \r\n",
        "                               self.device, \r\n",
        "                               src_vec_emb)\r\n",
        "        self.decoder = Decoder(output_vocab_size, \r\n",
        "                               args.output_embedding_dim,\r\n",
        "                               args.n_layers, \r\n",
        "                               args.n_heads,\r\n",
        "                               args.pf_dim,\r\n",
        "                               args.max_sent_len, \r\n",
        "                               args.dropout, \r\n",
        "                               self.device, \r\n",
        "                               trg_vec_emb)\r\n",
        "\r\n",
        "    def make_src_mask(self, src):\r\n",
        "        src_mask = (src != self.pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        return src_mask\r\n",
        "\r\n",
        "    def make_trg_mask(self, trg):\r\n",
        "        trg_pad_mask = (trg != self.pad_idx).unsqueeze(1).unsqueeze(2)\r\n",
        "        trg_len = trg.shape[1]\r\n",
        "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device=self.device)).bool()\r\n",
        "\r\n",
        "        trg_mask = trg_pad_mask & trg_sub_mask\r\n",
        "        return trg_mask\r\n",
        "\r\n",
        "    def forward(self, src, trg):\r\n",
        "        # src = src.permute(1, 0)\r\n",
        "        if trg is None:\r\n",
        "            trg = torch.zeros((src.shape[1], src.shape[0])).long().fill_(self.sos_idx).to(self.device)\r\n",
        "        # trg = trg.permute(1, 0)\r\n",
        "\r\n",
        "        src_mask = self.make_src_mask(src)\r\n",
        "        trg_mask = self.make_trg_mask(trg)\r\n",
        "        \r\n",
        "        enc_src = self.encoder(src, src_mask)\r\n",
        "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\r\n",
        "\r\n",
        "        output = output\r\n",
        "        attention = attention\r\n",
        "        \r\n",
        "        return output, attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLSXSTelZZQK"
      },
      "source": [
        "#train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZEY0LjSllwL"
      },
      "source": [
        "def epoch_time(start_time, end_time):\r\n",
        "    elapsed_time = end_time - start_time\r\n",
        "    elapsed_mins = int(elapsed_time / 60)\r\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\r\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPiFPuA0V-6h"
      },
      "source": [
        "def train_iter(model, iterator, epoch, optimizer, criterion, clip, args, checkpoint=None):\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    epoch_loss = 0\r\n",
        "    batch_ctr = 0\r\n",
        "\r\n",
        "    if checkpoint is not None:\r\n",
        "      batch_ctr = checkpoint['batch']\r\n",
        "      epoch_loss = checkpoint ['epoch_loss']\r\n",
        "    # print(iterator.__iter__())\r\n",
        "    for current_batch_ctr, batch in enumerate(iterator):\r\n",
        "        if current_batch_ctr < batch_ctr:\r\n",
        "            continue\r\n",
        "\r\n",
        "        torch.cuda.empty_cache()\r\n",
        "\r\n",
        "        src, src_len = batch[0]\r\n",
        "        trg = batch[1]\r\n",
        "        src = src.to(device)\r\n",
        "        trg = trg.to(device)\r\n",
        "        # print(src.shape, trg.shape)\r\n",
        "        optimizer.zero_grad()\r\n",
        "\r\n",
        "        output, _ = model(src, trg[:,:-1])\r\n",
        "        \r\n",
        "        output_dim = output.shape[-1]\r\n",
        "        output = output.contiguous().view(-1, output_dim)\r\n",
        "        trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "        loss = criterion(output, trg)\r\n",
        "        loss.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "\r\n",
        "        optimizer.step()\r\n",
        "        epoch_loss += loss.item()\r\n",
        "\r\n",
        "        if batch_ctr % 100 == 0 and args.save_checkpoint:\r\n",
        "            torch.save({\r\n",
        "                'epoch': epoch,\r\n",
        "                'batch': batch_ctr,\r\n",
        "                'model': model.state_dict(),\r\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\r\n",
        "                'epoch_loss': epoch_loss,\r\n",
        "                }, args.checkpoint_path)\r\n",
        "            EPOCH_INFO = f'Epoch: {epoch+1:02} | Batch: {batch_ctr+1:02}'\r\n",
        "            av_loss = epoch_loss/(batch_ctr+1)\r\n",
        "            LOSS_INFO = f'\\tRunning av training Loss: {av_loss:.3f} | Train PPL: {math.exp(av_loss):7.3f}'\r\n",
        "        \r\n",
        "        batch_ctr += 1\r\n",
        "    return epoch_loss/(batch_ctr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cy7A6uvdylT"
      },
      "source": [
        "def evaluate_iter(model, iterator, criterion, args):\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    epoch_loss = 0\r\n",
        "    batch_ctr = 0\r\n",
        "    with torch.no_grad():\r\n",
        "        for batch in iterator:\r\n",
        "            src, src_len = batch[0]\r\n",
        "            trg = batch[1]\r\n",
        "            src = src.to(device)\r\n",
        "            trg = trg.to(device)\r\n",
        "\r\n",
        "            output, _ = model(src, trg[:,:-1])\r\n",
        "            \r\n",
        "            output_dim = output.shape[-1]\r\n",
        "            output = output.contiguous().view(-1, output_dim)\r\n",
        "            trg = trg[:,1:].contiguous().view(-1)\r\n",
        "\r\n",
        "            loss = criterion(output, trg)\r\n",
        "\r\n",
        "            epoch_loss += loss.item()\r\n",
        "            batch_ctr += 1\r\n",
        "    return epoch_loss / (batch_ctr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCGGJRbGgkLB"
      },
      "source": [
        "def train(args):\r\n",
        "\r\n",
        "    TRG_MAX_LEN = args.trg_max_len\r\n",
        "    SRC_MAX_LEN = args.src_max_len\r\n",
        "\r\n",
        "    preprocessors = (vi_preprocessor, ja_preprocessor)\r\n",
        "\r\n",
        "    lengths = (SRC_MAX_LEN, TRG_MAX_LEN)\r\n",
        "    vocab_size = (args.input_vocab, args.output_vocab)\r\n",
        "\r\n",
        "    training_dataset = DataReader(args, args.training_data, preprocessors, vocab_size)\r\n",
        "    validation_dataset = DataReader(args, args.validation_data, preprocessors, DIC=training_dataset.vocab)\r\n",
        "\r\n",
        "    src_vec_dic = torch.stack([v for k, v in training_dataset.src_vec_dic.items()])\r\n",
        "    trg_vec_dic = torch.stack([v for k, v in training_dataset.trg_vec_dic.items()])\r\n",
        "    \r\n",
        "\r\n",
        "    INPUT_DIM = len(training_dataset.vocab.src_stoi)\r\n",
        "    OUTPUT_DIM = len(training_dataset.vocab.trg_stoi)\r\n",
        "    \r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "    PAD_IDX = training_dataset.vocab.src_stoi['<pad>']\r\n",
        "    SOS_IDX = training_dataset.vocab.src_stoi['<sos>']\r\n",
        "    EOS_IDX = training_dataset.vocab.src_stoi['<eos>']\r\n",
        "    \r\n",
        "    training_dataloader = DataLoader(training_dataset, batch_size = args.batch, drop_last=True, collate_fn=lambda b: collator(b,PAD_IDX,SRC_MAX_LEN,TRG_MAX_LEN))\r\n",
        "    validation_dataloader = DataLoader(validation_dataset, batch_size = args.batch, drop_last=True, collate_fn=lambda b: collator(b,PAD_IDX,SRC_MAX_LEN,TRG_MAX_LEN))\r\n",
        "    \r\n",
        "    model = Seq2Seq(args, INPUT_DIM, OUTPUT_DIM, PAD_IDX, SOS_IDX, EOS_IDX, src_vec_dic, trg_vec_dic).to(device)\r\n",
        "    \r\n",
        "    N_EPOCHS = args.epochs\r\n",
        "    CLIP = 1\r\n",
        "    best_valid_loss = float('inf')\r\n",
        "\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9,0.98))\r\n",
        "\r\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index= PAD_IDX)\r\n",
        "\r\n",
        "    start_epoch = 0\r\n",
        "    checkpoint = None\r\n",
        "    if args.load_checkpoint:\r\n",
        "        checkpoint = torch.load(args.checkpoint_path)\r\n",
        "        # print(checkpoint.keys())\r\n",
        "        model.load_state_dict(checkpoint['model'])\r\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\r\n",
        "        start_epoch = checkpoint['epoch']\r\n",
        "    \r\n",
        "    for epoch in range(start_epoch, N_EPOCHS):\r\n",
        "        start_time = time.time()\r\n",
        "\r\n",
        "        train_loss = train_iter(model, training_dataloader, epoch, optimizer, criterion, CLIP, args, checkpoint)\r\n",
        "        valid_loss = evaluate_iter(model, validation_dataloader, criterion, args)\r\n",
        "\r\n",
        "        end_time = time.time()\r\n",
        "\r\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n",
        "\r\n",
        "        # if valid_loss < best_valid_loss:\r\n",
        "        if True:\r\n",
        "            best_valid_loss = valid_loss\r\n",
        "            torch.save(model.state_dict(), args.save_model_path)\r\n",
        "            with open(args.save_dic_path, 'wb') as F:\r\n",
        "                pickle.dump(training_dataset.vocab, F)\r\n",
        "\r\n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\r\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\r\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\r\n",
        "        print('-----------------------------------------')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeJIJvn_d2zw"
      },
      "source": [
        "#inference function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f3sslbJncl4"
      },
      "source": [
        "def translate_sentence(model, vocab, sent, args, max_len=30):\r\n",
        "    model.eval()\r\n",
        "    \r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "    tokenized = ja_preprocessor(sent)\r\n",
        "    tokenized = ['<sos>'] + tokenized + ['<eos>']\r\n",
        "\r\n",
        "    numericalized = [vocab.src_stoi.get(t, 0) for t in tokenized]\r\n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(device)\r\n",
        "\r\n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(0).to(device)\r\n",
        "    mask = model.make_src_mask(tensor)\r\n",
        "    with torch.no_grad():\r\n",
        "        enc_src = model.encoder(tensor, mask)\r\n",
        "    trg_indexes = [vocab.trg_stoi['<sos>']]\r\n",
        "    for i in range(max_len):\r\n",
        "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\r\n",
        "        trg_mask = model.make_trg_mask(trg_tensor)\r\n",
        "        with torch.no_grad():\r\n",
        "            output, attention = model.decoder(trg_tensor, enc_src, trg_mask, mask)\r\n",
        "        pred_token = output.argmax(2)[:,-1].item()\r\n",
        "        trg_indexes.append(pred_token)\r\n",
        "        if pred_token == vocab.trg_stoi['<eos>']:\r\n",
        "            break\r\n",
        "    trg_tokens = [vocab.trg_itos[int(i)] for i in trg_indexes]\r\n",
        "    print(trg_indexes)\r\n",
        "    return trg_tokens[1:], attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zd-eVDQwcWGj"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "\r\n",
        "def display_attention(candidate, translation, attention):\r\n",
        "    fig = plt.figure(figsize=(10,10))\r\n",
        "    ax = fig.add_subplot(111)\r\n",
        "    \r\n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\r\n",
        "    \r\n",
        "    cax = ax.matshow(attention, cmap='bone')\r\n",
        "   \r\n",
        "    ax.tick_params(labelsize=15)\r\n",
        "    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in ja_preprocessor(candidate)] + ['<eos>'], \r\n",
        "                       rotation=45)\r\n",
        "    ax.set_yticklabels([''] + translation)\r\n",
        "\r\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "\r\n",
        "    plt.show()\r\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-KqDFHYgOsj"
      },
      "source": [
        "def inference(args, sent):\r\n",
        "    vocab = None\r\n",
        "    with open(args.load_dic_path, 'rb') as F:\r\n",
        "        vocab = pickle.load(F)\r\n",
        "    \r\n",
        "    INPUT_DIM = len(vocab.src_stoi)\r\n",
        "    OUTPUT_DIM = len(vocab.trg_stoi)\r\n",
        "    PAD_IDX = vocab.src_stoi['<pad>']\r\n",
        "    SOS_IDX = vocab.src_stoi['<sos>']\r\n",
        "    EOS_IDX = vocab.src_stoi['<eos>']\r\n",
        "\r\n",
        "    TRG_MAX_LEN = args.trg_max_len\r\n",
        "    SRC_MAX_LEN = args.src_max_len\r\n",
        "\r\n",
        "    preprocessors = (vi_preprocessor, ja_preprocessor)\r\n",
        "\r\n",
        "    lengths = (SRC_MAX_LEN, TRG_MAX_LEN)\r\n",
        "    vocab_size = (args.input_vocab, args.output_vocab)\r\n",
        "\r\n",
        "    training_dataset = DataReader(args, args.training_data, preprocessors, vocab_size)\r\n",
        "\r\n",
        "    src_vec_dic = torch.stack([v for k, v in training_dataset.src_vec_dic.items()])\r\n",
        "    trg_vec_dic = torch.stack([v for k, v in training_dataset.trg_vec_dic.items()])\r\n",
        "\r\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "    model = Seq2Seq(args,INPUT_DIM,OUTPUT_DIM, PAD_IDX, SOS_IDX, EOS_IDX, src_vec_dic, trg_vec_dic).to(device)\r\n",
        "    model.load_state_dict(torch.load(args.load_model_path,map_location=torch.device(device)))\r\n",
        "\r\n",
        "    sentence = sent\r\n",
        "    translation, attention = translate_sentence(model, vocab, sentence, args)\r\n",
        "    # with open(args.output_file, 'w', encoding='utf-8') as F:\r\n",
        "    #     print('Translated: ',' '.join(translation),file=F)\r\n",
        "    # display_attention(sentence,translation,attention)\r\n",
        "    print(translation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FadM5O_vpM0L"
      },
      "source": [
        "#Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FFQttstpL4L"
      },
      "source": [
        "import argparse\r\n",
        "\r\n",
        "def str2bool(v):\r\n",
        "    return v.lower() in ('true')\r\n",
        "\r\n",
        "def str2dict(v):\r\n",
        "    return {'run': str(v) }\r\n",
        "\r\n",
        "def str2tuple(v):\r\n",
        "    v = v.split('`!`!`')\r\n",
        "    return (v[0],v[1])\r\n",
        "\r\n",
        "parser = argparse.ArgumentParser()\r\n",
        "parser.add_argument(\"--batch\",type=int,default=32)\r\n",
        "parser.add_argument('--input_vocab',type=int,default=30000)\r\n",
        "parser.add_argument('--output_vocab',type=int,default=30000)\r\n",
        "parser.add_argument(\"--input_embedding_dim\",type=int,default=128)\r\n",
        "parser.add_argument(\"--output_embedding_dim\",type=int,default=128)\r\n",
        "# args.hidden_dim, args.kernel_sizes, args.max_sent_len\r\n",
        "parser.add_argument(\"--n_layers\",type=int,default=3)\r\n",
        "parser.add_argument(\"--n_heads\",type=int,default=8)\r\n",
        "parser.add_argument(\"--pf_dim\",type=int,default=512)\r\n",
        "\r\n",
        "parser.add_argument(\"--max_sent_len\",type=int,default=50)\r\n",
        "\r\n",
        "parser.add_argument(\"--dropout\",type=float,default=0.1)\r\n",
        "parser.add_argument(\"--epochs\",type=int,default=100)\r\n",
        "# parser.add_argument(\"--device\",type=str,default='auto',choices=['cpu', 'gpu','auto'])\r\n",
        "parser.add_argument('--exec_id',type=str2dict,default={'run': str(time.time()).replace('.','')})\r\n",
        "\r\n",
        "parser.add_argument('--training_data',type=str2tuple,default=('/content/drive/MyDrive/JaVi_Translation/data/train_tokenized.ja-vi.ja','/content/drive/MyDrive/JaVi_Translation/data/train_tokenized.ja-vi.vi'))\r\n",
        "parser.add_argument('--testing_data',type=str2tuple,default=('/content/drive/MyDrive/JaVi_Translation/data/test_tokenized.ja-vi.ja','/content/drive/MyDrive/JaVi_Translation/data/test_tokenized.ja-vi.vi'))\r\n",
        "parser.add_argument('--validation_data',type=str2tuple,default=('/content/drive/MyDrive/JaVi_Translation/data/test_tokenized.ja-vi.ja','/content/drive/MyDrive/JaVi_Translation/data/test_tokenized.ja-vi.vi'))\r\n",
        "\r\n",
        "parser.add_argument('--save_model_path',type=str,default='/content/drive/MyDrive/JaVi_Translation/trained_models/seq2seq_trans.pt')\r\n",
        "parser.add_argument('--save_dic_path',type=str,default='/content/drive/MyDrive/JaVi_Translation/trained_models/dictionary.pkl')\r\n",
        "parser.add_argument('--save_checkpoint',type=str2bool,default=True)\r\n",
        "parser.add_argument('--load_checkpoint',type=str2bool,default=False)\r\n",
        "parser.add_argument('--checkpoint_path',type=str,default='/content/drive/MyDrive/JaVi_Translation/trained_models/checkpoint_trans.pt')\r\n",
        "\r\n",
        "parser.add_argument('--load_model_path',type=str,default='/content/drive/MyDrive/JaVi_Translation/trained_models/seq2seq_trans.pt')\r\n",
        "parser.add_argument('--load_dic_path',type=str,default='/content/drive/MyDrive/JaVi_Translation/trained_models/dictionary.pkl')\r\n",
        "parser.add_argument('--src_max_len',type=int,default=50)\r\n",
        "parser.add_argument('--trg_max_len',type=int,default=50)\r\n",
        "parser.add_argument('--output_file',type=str,default='./translation_out.txt')\r\n",
        "\r\n",
        "\r\n",
        "args,unparsed = parser.parse_known_args()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnFwk2m9qjO3",
        "outputId": "276fa1f6-9b3d-442b-dae1-e897a7e029e4"
      },
      "source": [
        "train(args)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:62: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 5m 9s\n",
            "\tTrain Loss: 5.853 | Train PPL: 348.225\n",
            "\t Val. Loss: 5.351 |  Val. PPL: 210.791\n",
            "-----------------------------------------\n",
            "Epoch: 02 | Time: 5m 12s\n",
            "\tTrain Loss: 5.403 | Train PPL: 222.101\n",
            "\t Val. Loss: 5.186 |  Val. PPL: 178.790\n",
            "-----------------------------------------\n",
            "Epoch: 03 | Time: 5m 13s\n",
            "\tTrain Loss: 5.256 | Train PPL: 191.758\n",
            "\t Val. Loss: 5.081 |  Val. PPL: 160.998\n",
            "-----------------------------------------\n",
            "Epoch: 04 | Time: 5m 15s\n",
            "\tTrain Loss: 5.157 | Train PPL: 173.702\n",
            "\t Val. Loss: 5.018 |  Val. PPL: 151.174\n",
            "-----------------------------------------\n",
            "Epoch: 05 | Time: 5m 16s\n",
            "\tTrain Loss: 5.094 | Train PPL: 163.013\n",
            "\t Val. Loss: 4.974 |  Val. PPL: 144.670\n",
            "-----------------------------------------\n",
            "Epoch: 06 | Time: 5m 7s\n",
            "\tTrain Loss: 5.051 | Train PPL: 156.165\n",
            "\t Val. Loss: 4.957 |  Val. PPL: 142.211\n",
            "-----------------------------------------\n",
            "Epoch: 07 | Time: 4m 59s\n",
            "\tTrain Loss: 5.023 | Train PPL: 151.796\n",
            "\t Val. Loss: 4.926 |  Val. PPL: 137.802\n",
            "-----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOSOWr-DQzBk"
      },
      "source": [
        "tqdm(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vJl84ZBsLaG"
      },
      "source": [
        "a = inference(args, 'アンドレア ・ マージ が 開始 4 分 後 の トライ で イタリア に と っ て 最初 の 得点 を 入れ た 。')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnpthUMLQ21b"
      },
      "source": [
        "a = inference(args, '科学 オタク だっ た ん で す ね ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmFgZv0O-qbB"
      },
      "source": [
        "sent = 'アンドレア ・ マージ が 開始 の 得点 を 入れ た 。'\r\n",
        "\r\n",
        "vocab = None\r\n",
        "with open(args.load_dic_path, 'rb') as F:\r\n",
        "    vocab = pickle.load(F)\r\n",
        "\r\n",
        "INPUT_DIM = len(vocab.src_stoi)\r\n",
        "OUTPUT_DIM = len(vocab.trg_stoi)\r\n",
        "PAD_IDX = vocab.src_stoi['<pad>']\r\n",
        "SOS_IDX = vocab.src_stoi['<sos>']\r\n",
        "EOS_IDX = vocab.src_stoi['<eos>']\r\n",
        "\r\n",
        "TRG_MAX_LEN = args.trg_max_len\r\n",
        "SRC_MAX_LEN = args.src_max_len\r\n",
        "\r\n",
        "preprocessors = (vi_preprocessor, ja_preprocessor)\r\n",
        "\r\n",
        "lengths = (SRC_MAX_LEN, TRG_MAX_LEN)\r\n",
        "vocab_size = (args.input_vocab, args.output_vocab)\r\n",
        "\r\n",
        "training_dataset = DataReader(args, args.training_data, preprocessors, vocab_size)\r\n",
        "\r\n",
        "src_vec_dic = torch.stack([v for k, v in training_dataset.src_vec_dic.items()])\r\n",
        "trg_vec_dic = torch.stack([v for k, v in training_dataset.trg_vec_dic.items()])\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "model = Seq2Seq(args,INPUT_DIM,OUTPUT_DIM, PAD_IDX, SOS_IDX, EOS_IDX, src_vec_dic, trg_vec_dic).to(device)\r\n",
        "model.load_state_dict(torch.load(args.load_model_path,map_location=torch.device(device)))\r\n",
        "\r\n",
        "sentence = sent\r\n",
        "translation, attention = translate_sentence(model, vocab, sentence, args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRvR_HBv4mpf"
      },
      "source": [
        "translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiNSTMgQInt7"
      },
      "source": [
        "TRG_MAX_LEN = args.trg_max_len\r\n",
        "SRC_MAX_LEN = args.src_max_len\r\n",
        "\r\n",
        "preprocessors = (vi_preprocessor, ja_preprocessor)\r\n",
        "\r\n",
        "lengths = (SRC_MAX_LEN, TRG_MAX_LEN)\r\n",
        "vocab_size = (args.input_vocab, args.output_vocab)\r\n",
        "\r\n",
        "training_dataset = DataReader(args, args.training_data, preprocessors, vocab_size)\r\n",
        "validation_dataset = DataReader(args, args.validation_data, preprocessors, DIC=training_dataset.vocab)\r\n",
        "\r\n",
        "src_vec_dic = torch.stack([v for k, v in training_dataset.src_vec_dic.items()])\r\n",
        "trg_vec_dic = torch.stack([v for k, v in training_dataset.trg_vec_dic.items()])\r\n",
        "\r\n",
        "\r\n",
        "INPUT_DIM = len(training_dataset.vocab.src_stoi)\r\n",
        "OUTPUT_DIM = len(training_dataset.vocab.trg_stoi)\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "\r\n",
        "PAD_IDX = training_dataset.vocab.src_stoi['<pad>']\r\n",
        "SOS_IDX = training_dataset.vocab.src_stoi['<sos>']\r\n",
        "EOS_IDX = training_dataset.vocab.src_stoi['<eos>']\r\n",
        "\r\n",
        "training_dataloader = DataLoader(training_dataset, batch_size = args.batch, drop_last=True, collate_fn=lambda b: collator(b,PAD_IDX,SRC_MAX_LEN,TRG_MAX_LEN))\r\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size = args.batch, drop_last=True, collate_fn=lambda b: collator(b,PAD_IDX,SRC_MAX_LEN,TRG_MAX_LEN))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvv2xTHnJSDm"
      },
      "source": [
        "for i, e in enumerate(validation_dataloader):\r\n",
        "    src, src_len = e[0]\r\n",
        "    trg = e[1]\r\n",
        "    print([vocab.trg_itos[int(i)] for i in trg.T[0]])\r\n",
        "    print(trg.T[0])\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cN41Uf1LIVd6"
      },
      "source": [
        "translation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ND-kjEMAvMiB"
      },
      "source": [
        "x = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPUwVDpm9u7M"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R35-Uk549w0i"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-oeJUXgXq02"
      },
      "source": [
        "!wget https://thiaisotajppub.s3-ap-northeast-1.amazonaws.com/publicfiles/baomoi.window2.vn.model.bin.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sy6HF4EGYSim"
      },
      "source": [
        "!cp baomoi.window2.vn.model.bin /content/drive/MyDrive/JaVi_Translation/w2v_pretrained"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}